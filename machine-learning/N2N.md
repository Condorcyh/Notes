## N2N 学习论文解析

### 标题

N2N LEARNING: NETWORK TO NETWORK COMPRESSION VIA POLICY GRADIENT REINFORCEMENT LEARNING

使用策略梯度的方法进行从网络到网络的模型压缩。

### 摘要

当前的深层神经仍然被硬件和速度深深的限制着。 传统的模型尝试去用启发式的方法压缩修改压缩框架。因为所有的架构搜索空间非常巨大，修改深层神经网络的架构十分困难。 所以在这篇文章中我们使用强化学习的方式来解决这个问题。  我们的方法接受一个比较大的老师网络作为输入并且输出一个压缩之后的学生网络。 在我们的方法第一步，使用循环神经网络来不断的从大的 老师网络中移除层，在第二部中，另一个循环神经网络对每一层进行压缩大小。 最终的结果网络使用一个 reward 来进行评估，这个 reward 根据准确率和压缩率的得出。 我们的方法利用这个 reward 值去找到一个最优的学生网络。



### 方法

#### 1. 马尔可夫决定过程

我们把找到一个压缩之后的架构看做成一个一系列的决策过程。 这个过程就是马尔可夫决策过程。 正式的讲，马尔可夫决策过程被定义成 三元组{S, A, T, r, y}

##### 状态

S 是状态空间，是一个有限的包含了全部可能从老师网络中推导出的网络架构集合。 比如说 VGG 网络代表了初始状态 s,  通过在第一层去除掉一个卷积核，我们得到了一个新的网络架构 s'.

##### 动作

A 是一个有限的动作集合 可以把 一个网络架构变成另一个网络架构。 在我们方法中，有两种动作类型： 去除一层 和 去除一层中的参数。后面会单独描述这两种动作类型。

##### 转换函数

S * A -> S 是动态的状态转换，这个转换被称作是 T，这个函数实际上是确定性的。

##### 折扣因子

Y 是折扣因子，我们把 y 设置成1，所以这样所有的 reward 的值和实际返回的值（起效果的值）是相等的。

##### 奖赏

 r:  S->R 是一个 奖赏函数。 这个网络架构的奖赏可以被翻译成一个和给定网络架构 s 相关联的分数。 注意我们对于临时状态定义了 0 作为 reward,代表了不完成状态。 后面也会单独讲奖励函数。



#### 2.  学生-老师 强化学习

在 MDP设定下， 强化学习的任务已经变成了学习一个优化的策略 π：S->A, 这个策略可以在整个网络下取得一个最大值。  

全部的 reward 最终会被定义为每一个 reward 求和。

我们采用一个 策略梯度强化学习方法并且迭代的更新策略根据对于 reward 的采样估计。 对于动作空间的设计对于 策略梯度方法 去有效的搜索状态空间是十分重要的。 如果这个动作以一种十分增量式的方法被选择，对于神经网络做出很大的改变会需要一个很长的序列。 为了解决这个问题，我们提出了一个两步的强化学习过程。 在第一步我们的策略去决定是否要保留一个特定的层。 在第二步，一个不同的策略选择一些剩下的层该保留哪些变量的决策。 这样我们就能够有效的去探索我们的状态空间了。

还有一个详细的算法图来介绍这个算法的具体。

##### 去除层

在去除层的阶段，动作 At 用来决定是保留还是去除一层。 这个操作的动作轨迹长度是L，L是神经网络的总层数。 在去除层的每一步t，双向 LSTM 策略观察到了隐层状态， ht-1 和 ht+1 还有 xt 关于当前的层。最终的决策函数是 π（at|ht-1,ht+1,xt）关于当前的层 l 的信息是

Xt = (l,k,s,p,n,S(start),S(end))

l 是当前层的类型， k 是核大小， s 是步长， p 是padding，n 是输出的数量。  可以看出这仅仅是对于卷积层的操作？

##### 压缩层

对于一层的收缩策略轨迹的长度是这一层中的可配置变量的数量。 在收缩收缩阶段的每一个步骤 t, 这个策略观察到了隐含的状态 ht-1, 这个之前采样好的动作 at-1 和当前的层信息 xt,  这样构成了一个相类似的策略。

##### 奖赏函数

奖赏函数的设计对于学习策略是至关重要的。 一个设计的不好的奖赏函数不能够有效的区分好的网络架构和不好的网络架构。

模型压缩的目标是去最大化压缩同时还能保持一个很高的准确率。 我们的奖赏函数对于高压缩率，低准确率模型的惩罚要大于高准确率，低压缩率的模型。
$$
R = R_c \times R_a
= C(2-C)\times \frac {A}{A_t}
$$
C 是学生模型的相对压缩率， A是学生模型的验证准确率，At 是老师模型的验证准确率，作为一个常数被提供。 Rc 和 Ra 分别对应于压缩和准确率的奖赏函数。 

##### 奖赏函数的约束

可以把一些硬件和资源约束加入到 奖赏函数的定义中，然后 reward 就会变成一个分段函数，



##### 优化

我们现在对我们的每一个随机策略来描述优化过程。因为两个的优化过程是完全一样的，所以我们统一使用π来进行代表。 对于每一个策略网络会有自己不同的 θ参数。

我们的目标函数是 对于一系列行动的 reward 的期望值。
$$
\bigtriangledown_{\theta}J(\theta) = \bigtriangledown_{\theta}E_{a_{1:t}-p_\theta(R)}
$$
上面其实就是一些列的 policy-gredient 的公式，还要再复习一下 policy-gredient



##### 知识蒸馏

学生网络在训练过程中用到的数据是由老师网络标注的，而不是去使用硬标记，我们使用非正则化的对数概率值。  

这个蒸馏的本质其实就是在学生的损失函数中加入关于针对老师网络的单独的项。